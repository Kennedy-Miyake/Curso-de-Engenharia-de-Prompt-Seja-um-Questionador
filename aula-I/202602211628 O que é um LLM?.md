---
course: "[[notes/Curso de Engenharia de Prompt — Seja um Questionador]]"
section: aula-i/
create_date: 2026-02-21
last_modify: Saturday 21st February 2026 16:29:42
---

# 202602211628 O que é um LLM?
---

# Original
## O que são LLMs?
### O que é a "IA" que tanto dizem
- Large Language Model, ou Modelo de Linguagem Grande são modelos de aprendizado profundo baseado em redes neurais.
- Treinado com conjuntos massivos de dados textuais para compreender, processar e gerar textos em linguagem natural.
- Funcionam processando entradas de texto (prompts) e produzindo saídas preditivas, simulando compreensão semântica por meio de padrões aprendidos.

### Por que Existem Tantos Modelos?
- Competição entre empresas. E as vezes para adaptação a contextos específicos.
- Modelos são treinados com foco em contextos específicos como código, imagens, raciocínio ético, etc.
- Claude é para geração de código, Grok para conversação sem restrtições, ChatGPT para contexto geral, etc.

### Como os Modelos nos Respondem? Mágica?
- Respostas advém de um processo de predição sequencial e estocástico.
- Cada palavra (ou token) é gerada com base na probabilidade condicional das anteriores, semelhante a um auto-complete no celular.
- Semelhante também a uma Cadeia de Markov, mas ao invés de assumir o estado presente, ele escala para contextos longos.
- Processos estocásticos introduzem "criatividade" via ruído aleatório (existindo uma entropia)

### O que Está por Baixo dos Panos?
- Muitos números, muitos vetores, muito cálculo, muito poder de processamento, muito calor, etc.
- "Por dentro do capô" os LLMs baseiam-se na arquitetura Transformer, introduzida em 2017 pelo paper "Attention is All You Need" por pesquisadores da Google.
- Uma grande revolução ao substituir Redes Neurais Recorrentes (RNNs) por mecanismos de atenção paralelizáveis.
- RRNs processavam texto de forma sequencial, criando limitações e o chamado 'gargalo sequencial'.
- Transformers processam sequências inteiras de texto em paralelo usando 'atenção'.

### Como os Modelos Aprendem?
- Aprendem via treinamento supervisionado massivo em três fases principais.
- 1. Pré-treinamento: Exposição a dados não rotulados para prever tokens mascarados ou próximos, aprendendo gramática, fatos gerais e diversos outros padrões de escrita.
- 2. Fine-tuning: Ajuste supervisionado em tarefas específicas com dados rotulados.
- 3. RLHF (Reinforcemente Learning from Human Feedback): Otimização via feedback humano para alinhar saídas a preferência de quem o treina.

### "Modelos de IA" São Inteligentes?
- NÃO.
- Quaisquer LLM que você use, todos eles continuam sendo "papagaios estocásticos" ou "geradores de texto glorificados".
- Independente do quão 'inteligentes' pareçam, não há nenhum raciocínio, lógica ou inferência, apenas probabilidades.
- MODELOS DE LLM SÃO APENAS GERADORES DE TEXTO PROBABILISTICOS GLORIFICADOS, APENAS.

---

# Deprecated
## O que é um LLM?
- LLM significa _Large Language Model_ (Grande Modelo de Linguagem). São modelos treinados com um enorme volume de dados da internet e projetado para processar, compreender e gerar texto em linguagem natural.
- Exemplos de LLMs: GPT-$5$, Gemini $4.1$, DeepSeek V$3.2$, etc.
- **Como funciona**:
	- Baseia-se em arquitetura Transformer (introduzida por pesquisadores afiliados ao Google). Processando texto em tokens (pedaços de palavras) e usa "atenção" para relacionar partes do input.
	- No treinamento o modelo aprende prevendo a próxima palavra em sequências massivas de dados.
	- Existem limitações, a mais comum são as "alucinações", no qual o modelo tende a inventar fatos.
- Modelos de LLM **NÃO SÃO INTELIGENTES**. São apenas geradores de texto probabilisticos / estatísticos sofisticados.
- LLMs são ferramentas de trabalho diário, aprender manipulá-los te otimizará tempo de pesquisas intensivas, tarefas repetitivas, escrita, etc.

---

# References
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [Transformers: A Arquitetura que revolucionou a IA Contemporânea](https://eailab.labmax.org/2026/02/05/transformers-a-arquitetura-que-revolucionou-a-ia-contemporanea/)
- [Large Language Models explained briefly | 3Blue1Brown](https://youtu.be/LPZh9BOjkQs?si=NYoDqO-H4oAZ19on)
- [Attention in transformers, step-by-step | Deep Learning Chapter 6 | 3Blue1Brown](https://youtu.be/eMlx5fFNoYc?si=ZXuDVaUSpRO7HOj9)
- [Transformers, the tech behind LLMs | Deep Learning Chapter 5 | 3Blue1Brown](https://youtu.be/wjZofJX0v4M?si=aMDxcoLHUuOp4yJg)
- [A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://arxiv.org/abs/2311.05232)